<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Fed-WSVAD Documentation</title>
<style>
body { margin: 0; font-family: Arial, sans-serif; display: flex; }
nav { width: 220px; background: #f8f8f8; padding: 20px; box-sizing: border-box; border-right: 1px solid #ddd; height: 100vh; overflow: auto; position: fixed; top: 0; }
nav ul { list-style: none; padding: 0; margin: 0; }
nav li { margin-bottom: 8px; }
nav a { text-decoration: none; color: #333; font-weight: bold; }
nav a:hover { text-decoration: underline; }
main { margin-left: 260px; padding: 20px; max-width: 800px; }
h1 { font-size: 24px; margin-bottom: 0.5em; }
h2 { font-size: 20px; margin-top: 1.5em; border-bottom: 1px solid #ddd; padding-bottom: 0.3em; }
h3 { font-size: 18px; margin-top: 1.2em; }
h4 { font-size: 16px; margin-top: 1em; }
pre { background: #f4f4f4; padding: 10px; border: 1px solid #ddd; border-radius: 4px; position: relative; }
.copy-button { position: absolute; top: 8px; right: 8px; padding: 2px 6px; font-size: 12px; border: 1px solid #ccc; background: #eee; cursor: pointer; border-radius: 3px; }
.copy-button:hover { background: #ddd; }
ul, ol { margin-top: 0; }
</style>
</head>
<body>
<nav>
  <ul>
    <li><a href="#home">Home</a></li>
    <li><a href="#wsvad">WSVAD</a></li>
    <li><a href="#fed-wsvad">Federated Setup (Fed-WSVAD)</a></li>
    <li><a href="#jetson">Jetson AGX Xavier Deployment</a></li>
    <li><a href="#feature-extraction">Feature Extraction Guide (VideoMAE V2 Backbone)</a></li>
  </ul>
</nav>
<main>
  <h1 id="home">Dual-detector Re-optimization for Federated Video Anomaly Detection via Adaptive Dynamic Recursive Mapping</h1>
  <p align="center"><strong>⭐ If you find our code useful, please consider starring this repository!</strong></p>
  <hr>
  <p align="center">
    <img src="https://github.com/rekkles2/Fed_WSVAD/raw/main/Figure/model.svg" alt="Figure 1: Overview of the proposed dual-detector framework for Fed-WSVAD" style="width:80%; height:auto;">
    <br>
    <em>Figure 1: Overview of the proposed dual-detector framework for federated weakly supervised video anomaly detection (Fed-WSVAD).</em>
  </p>
  <hr>
  <h3>Key Contributions</h3>
  <ul>
    <li>We introduce a dual-detector framework that leverages adaptive dynamic recursive mapping to generate more stable anomaly scores, thereby enhancing detection accuracy.</li>
    <li>We introduce the SSALA algorithm to learn private local models that adapt to each client’s data, mitigating the effects of scene heterogeneity.</li>
    <li>We demonstrate superior detection performance and robustness through experiments on two benchmarks, validating the effectiveness of the proposed framework.</li>
  </ul>

  <h2 id="wsvad">WSVAD</h2>
  <h3>Setup Instructions</h3>
  <ol>
    <li><strong>Clone the repository:</strong>
      <pre><code>git clone https://github.com/rekkles2/Fed_WSVAD.git
cd Fed_WSVAD</code></pre>
    </li>
    <li><strong>Create and activate the Conda environment:</strong> (Check the <code>VAD/environment.yml</code> file for the environment name, e.g., <code>vad_env</code>)
      <pre><code>conda env create -f VAD/environment.yml
conda activate &lt;your_environment_name&gt;  # e.g., conda activate vad_env</code></pre>
    </li>
  </ol>
  <hr>
  <h3>Running</h3>
  <pre><code>python main.py</code></pre>
  <hr>
  <h3>Evaluation</h3>
  <p>To evaluate the pretrained model (e.g., on ShanghaiTech), run the following command:</p>
  <pre><code># Ensure the path to the .pkl model file is correct.
python VAD/inference.py --inference_model='shanghaitech.pkl'</code></pre>

  <h2 id="fed-wsvad">Federated Setup (Fed-WSVAD)</h2>
  <p>This guide provides comprehensive instructions for setting up an end-to-end federated video anomaly detection (Fed-WSVAD) framework. Follow these steps to successfully deploy and execute your federated learning experiments.</p>
  <hr>
  <h3>1. Preparation</h3>
  <p>Before you begin, ensure all necessary hardware, software, and data are in place. Careful preparation is key to a smooth setup process.</p>
  <h4>1.1 Hardware Requirements</h4>
  <ul>
    <li><strong>Server:</strong> You will need <strong>one machine</strong> (a standard PC or server) to host the central aggregation server.</li>
    <li><strong>Clients:</strong> At least <strong>two NVIDIA Jetson AGX Xavier</strong> devices are required to function as the distributed clients.</li>
    <li><strong>Network:</strong> A reliable <strong>Wi-Fi network</strong> is essential to connect the server and all client devices for communication.</li>
  </ul>
  <h4>1.2 Software &amp; Data Setup</h4>
  <ul>
    <li><strong>Dependencies:</strong> Install all required libraries and dependencies listed in the <code>requirements.txt</code> file.</li>
    <li><strong>Data Preparation:</strong> Download and preprocess the video datasets as needed. Ensure the data is formatted and accessible to both the server and clients.</li>
  </ul>
  <h4>1.3 Configuration Steps</h4>
  <ul>
    <li><strong>Server Address Modification:</strong> Locate and modify the relevant IP address in the source code files: <code>Fed_VAD/server.py</code> and <code>Fed_VAD/client_pytorch.py</code>.</li>
    <li><strong>Replace Placeholder IP:</strong> Update the placeholder IP address (e.g., <code>192.168.1.100:8080</code>) with the actual server address. This ensures clients can connect to the server.</li>
  </ul>
  <hr>
  <h3>2. Scene Similarity Analysis</h3>
  <ul>
    <li><strong>Purpose:</strong> Prepare each scene photo (without humans) to compute scene similarity. The most similar scenes might be chosen for paired experiments.</li>
    <li><strong>Execution:</strong> Run the following script:
      <pre><code>python Fed_VAD/Scene_Similarity.py</code></pre>
    </li>
  </ul>
  <hr>
  <h3>3. Running the Framework</h3>
  <h4>3.1 Start the Server</h4>
  <p>Begin by starting the central server process on your designated server machine:</p>
  <pre><code># --rounds: total number of federated learning rounds.
python Fed_VAD/server.py --rounds=10</code></pre>
  <h4>3.2 Start Each Client</h4>
  <p>After the server is running, start the client process on each client device. For each client, open a terminal and run:</p>
  <pre><code># --cid: unique Client ID for this device.
python Fed_VAD/client_pytorch.py --cid=&lt;CLIENT_ID&gt;</code></pre>
  <p>Replace <code>&lt;CLIENT_ID&gt;</code> with the appropriate ID (e.g., 0, 1, 2, ...).</p>
  <hr>
  <p>By following these steps, you should be able to successfully run the Fed-WSVAD framework. Good luck with your experiments!</p>

  <h2 id="jetson">Jetson AGX Xavier Deployment Guide</h2>
  <p><strong>Status:</strong> <em>In preparation</em> (expected release in 2 months)</p>
  <blockquote>
    <strong>Configuring the Jetson AGX Xavier for federated learning is quite complex.</strong>
    A detailed setup tutorial is currently being prepared and will be released within the next two months—stay tuned.
  </blockquote>

  <h2 id="feature-extraction">Feature Extraction Guide (VideoMAE V2 Backbone)</h2>
  <p>A modular toolkit for extracting video features using the <strong>Video Masked Autoencoder V2 (VideoMAE V2)</strong> framework. This backbone is part of the repository <a href="https://github.com/rekkles2/Fed_WSVAD">rekkles2/Fed_WSVAD</a>.</p>
  <h3>Introduction</h3>
  <p>This repository provides scripts for extracting video features using the Video-MAE V2 model. It is designed to support feature preprocessing in weakly supervised video anomaly detection tasks.</p>
  <h3>Pretrained Model</h3>
  <p>Download the pretrained Video-MAE V2 checkpoint:</p>
  <p><a href="https://drive.google.com/file/d/1xr1yeA2cxck4NCLX1qjAi3JU9qhRpfGr/view?usp=drive_link">Download Pretrained Model</a></p>

</main>
<script>
document.querySelectorAll('pre').forEach(function(pre) {
  var btn = document.createElement('button');
  btn.className = 'copy-button';
  btn.textContent = 'Copy';
  btn.addEventListener('click', function() {
    var code = pre.querySelector('code');
    if (code) {
      navigator.clipboard.writeText(code.innerText).then(function() {
        btn.textContent = 'Copied!';
        setTimeout(function() { btn.textContent = 'Copy'; }, 2000);
      });
    }
  });
  pre.appendChild(btn);
});
</script>
</body>
</html>
